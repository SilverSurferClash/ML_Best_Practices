{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, roc_curve, log_loss, make_scorer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, FunctionTransformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.decomposition import PCA, NMF\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.manifold import TSNE\nfrom umap import UMAP\nfrom scipy.cluster.hierarchy import dendrogram, ward\n\nimport optuna\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\n\nsns.set_theme(style = 'white', palette = 'viridis')\npal = sns.color_palette('viridis')\n\npd.set_option('display.max_rows', 100)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:33.196376Z","iopub.execute_input":"2023-04-29T16:23:33.197837Z","iopub.status.idle":"2023-04-29T16:23:43.875554Z","shell.execute_reply.started":"2023-04-29T16:23:33.197777Z","shell.execute_reply":"2023-04-29T16:23:43.874324Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## How to define the different states of the data","metadata":{}},{"cell_type":"code","source":"#Variable for the imported training data after cleaning up - \"train\"\n#Variable for the feature data set - \"train_X\"\n#Variable for the target series- \"train_y\"\n#Variable for the  target colum - \"target\"\n#Variable for the imported test data set - \"test\"\n#Variable for the transformed test data set - \"test_trans\"\n#Variable for the training data set in the cross validation loop - \"X_train\"\n#Variable for the validation data set in the cross validation loop - \"X_val\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:43.881902Z","iopub.execute_input":"2023-04-29T16:23:43.882209Z","iopub.status.idle":"2023-04-29T16:23:43.887338Z","shell.execute_reply.started":"2023-04-29T16:23:43.882162Z","shell.execute_reply":"2023-04-29T16:23:43.886237Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(r'../input/playground-series-s3e13/train.csv')\ntest_1 = pd.read_csv(r'../input/playground-series-s3e13/test.csv')\norig_train = pd.read_csv(r'../input/vector-borne-disease-prediction/trainn.csv')\n\ntrain.drop('id', axis = 1, inplace = True)\ntest = test_1.drop('id', axis = 1)\n\ntarget = 'prognosis'","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:43.888691Z","iopub.execute_input":"2023-04-29T16:23:43.888986Z","iopub.status.idle":"2023-04-29T16:23:43.928769Z","shell.execute_reply.started":"2023-04-29T16:23:43.888958Z","shell.execute_reply":"2023-04-29T16:23:43.927777Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train, orig_train])\nprint(f'There are {train.duplicated(subset = list(train)[0:-1]).value_counts()[0]} non-duplicate values out of {train.count()[0]} rows in original train dataset')","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:43.932630Z","iopub.execute_input":"2023-04-29T16:23:43.933405Z","iopub.status.idle":"2023-04-29T16:23:43.954022Z","shell.execute_reply.started":"2023-04-29T16:23:43.933363Z","shell.execute_reply":"2023-04-29T16:23:43.952939Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"There are 959 non-duplicate values out of 959 rows in original train dataset\n","output_type":"stream"}]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:43.955356Z","iopub.execute_input":"2023-04-29T16:23:43.955772Z","iopub.status.idle":"2023-04-29T16:23:43.964149Z","shell.execute_reply.started":"2023-04-29T16:23:43.955739Z","shell.execute_reply":"2023-04-29T16:23:43.962811Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(959, 65)"},"metadata":{}}]},{"cell_type":"code","source":"train_X = train.drop(target, axis = 1).copy()\ntrain_y = train[target]\n\n#Fix the issues that the original data set used \"-\"to separate the words\ntrain_y = [prognosis.replace(' ', '_') for prognosis in train_y]\ntrain_y = np.array(train_y)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:43.965531Z","iopub.execute_input":"2023-04-29T16:23:43.965914Z","iopub.status.idle":"2023-04-29T16:23:43.973596Z","shell.execute_reply.started":"2023-04-29T16:23:43.965868Z","shell.execute_reply":"2023-04-29T16:23:43.972402Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_X.shape, train_y.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:43.975399Z","iopub.execute_input":"2023-04-29T16:23:43.975830Z","iopub.status.idle":"2023-04-29T16:23:43.984951Z","shell.execute_reply.started":"2023-04-29T16:23:43.975784Z","shell.execute_reply":"2023-04-29T16:23:43.983783Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"((959, 64), (959,))"},"metadata":{}}]},{"cell_type":"code","source":"#Cross validation strategy\nseed = 42\nsplits = 3\n#cv = RepeatedStratifiedKFold(n_splits = splits, n_repeats = 5, random_state = seed)\ncv = StratifiedKFold(n_splits = splits, random_state = seed, shuffle = True)\n\nnp.random.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:43.986471Z","iopub.execute_input":"2023-04-29T16:23:43.986937Z","iopub.status.idle":"2023-04-29T16:23:43.994760Z","shell.execute_reply.started":"2023-04-29T16:23:43.986895Z","shell.execute_reply":"2023-04-29T16:23:43.993770Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def apk(actual, predicted, k=10):\n    \n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i,p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    if not actual:\n        return 0.0\n\n    return score / min(len(actual), k)\n\n\ndef mapk(actual, predicted, k=10):\n    \n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:43.996059Z","iopub.execute_input":"2023-04-29T16:23:43.996399Z","iopub.status.idle":"2023-04-29T16:23:44.005433Z","shell.execute_reply.started":"2023-04-29T16:23:43.996366Z","shell.execute_reply":"2023-04-29T16:23:44.004390Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def cross_val_pipe(model, train_X = train_X, train_y = train_y, target ='prognosis', cv = cv, label = ''):\n    \n    \n    #creating encoder and transforming prognosis\n    enc = LabelEncoder()\n    train_y = enc.fit_transform(train_y)\n    \n    #initiate prediction arrays and score lists\n    val_predictions = np.zeros((len(train_X), 11)) # Validation predictions are stored in a matrix with length of the number of trainijng samples and # of preds\n    train_predictions = np.zeros((len(train_X), 11)) # Same for the train predictions\n    train_logloss, val_logloss = [], [] #Store the results from the log_loss calc in a list\n    train_map3, val_map3 = [], [] #Store the results from the log_loss calc in a list\n    \n    #training model, predicting prognosis probability, and evaluating log loss\n    for fold, (train_idx, val_idx) in enumerate(cv.split(train_X, train_y)):\n                \n        model.fit(train_X.iloc[train_idx], train_y[train_idx])\n        \n        train_preds = model.predict_proba(train_X.iloc[train_idx])\n        val_preds = model.predict_proba(train_X.iloc[val_idx])\n                  \n        train_predictions[train_idx] += train_preds\n        val_predictions[val_idx] += val_preds\n        \n        train_score = log_loss(train_y[train_idx], train_preds)\n        val_score = log_loss(train_y[val_idx], val_preds)\n        \n        train_logloss.append(train_score)\n        val_logloss.append(val_score)\n        \n        #select three most probable prognosis based on train dataset prediction\n        train_index = np.argsort(-train_preds)[:,:3] #return index of three most probable prognosis\n        \n        #select three most probable prognosis based on validation dataset prediction\n        val_index = np.argsort(-val_preds)[:,:3]\n    \n        #calculate map@3\n        train_score = mapk(train_y[train_idx].reshape(-1, 1), train_index, 3)\n        val_score = mapk(train_y[val_idx].reshape(-1, 1), val_index, 3)\n        print(f\" The val_score for {fold} is {val_score}\")\n        \n        train_map3.append(train_score)\n        val_map3.append(val_score)\n    \n    print(f'Val log_loss   : {np.mean(val_logloss):.5f} | Train log_loss   : {np.mean(train_logloss):.5f} | {label}')\n    print(f'Val MAP@3 Score: {np.mean(val_map3):.5f} | Train MAP@3 Score: {np.mean(train_map3):.5f} | {label}\\n')\n    \n    return val_logloss, val_map3","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:44.006931Z","iopub.execute_input":"2023-04-29T16:23:44.007388Z","iopub.status.idle":"2023-04-29T16:23:44.020060Z","shell.execute_reply.started":"2023-04-29T16:23:44.007347Z","shell.execute_reply":"2023-04-29T16:23:44.018943Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Classification models\n\n#List of tuples\nmodels = [\n    ('log', LogisticRegression(random_state = seed, max_iter = 10000)),\n    #('svc', SVC(random_state = seed, probability = True)),\n    #('lda', LinearDiscriminantAnalysis()),\n    #('qda', QuadraticDiscriminantAnalysis()),\n    #('gauss', GaussianProcessClassifier(random_state = seed)),\n    #('et', ExtraTreesClassifier(random_state = seed)),\n    #('rf', RandomForestClassifier(random_state = seed)),\n    #('xgb', XGBClassifier(random_state = seed, objective = 'multi:softprob', eval_metric = 'map@3')),\n    ('lgb', LGBMClassifier(random_state = seed, objective = 'softmax', metric = 'softmax')),\n    #('dart', LGBMClassifier(random_state = seed, objective = 'softmax', metric = 'softmax', boosting_type = 'dart')),\n    #('cb', CatBoostClassifier(random_state = seed, objective = 'MultiClass', verbose = 0)),\n    #('gb', GradientBoostingClassifier(random_state = seed)),\n    #('hgb', HistGradientBoostingClassifier(random_state = seed)),\n    #('ada', AdaBoostClassifier(random_state = seed)),\n    #('knn', KNeighborsClassifier())\n]","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:44.021589Z","iopub.execute_input":"2023-04-29T16:23:44.022178Z","iopub.status.idle":"2023-04-29T16:23:44.031453Z","shell.execute_reply.started":"2023-04-29T16:23:44.022143Z","shell.execute_reply":"2023-04-29T16:23:44.030382Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Execution","metadata":{}},{"cell_type":"code","source":"class Decomp:\n    def __init__(self, n_components, method=\"pca\", scaler_method='standard'):\n        self.n_components = n_components\n        self.method = method\n        self.scaler_method = scaler_method\n        \n    def dimension_reduction(self, df):\n            \n        X_reduced = self.dimension_method(df)\n        df_comp = pd.DataFrame(X_reduced, columns=[f'{self.method.upper()}_{_}' for _ in range(self.n_components)], index=df.index)\n        \n        return df_comp\n    \n    def dimension_method(self, df):\n        X = self.scaler(df)\n        if self.method == \"pca\":\n            comp = PCA(n_components=self.n_components, random_state=0)\n            X_reduced = comp.fit_transform(X)\n        elif self.method == \"nmf\":\n            comp = NMF(n_components=self.n_components, random_state=0)\n            X_reduced = comp.fit_transform(X)\n        elif self.method == \"umap\":\n            comp = UMAP(n_components=self.n_components, random_state=0)\n            X_reduced = comp.fit_transform(X)\n        elif self.method == \"tsne\":\n            comp = TSNE(n_components=self.n_components, random_state=0) # Recommend n_components=2\n            X_reduced = comp.fit_transform(X)\n        else:\n            raise ValueError(f\"Invalid method name: {method}\")\n        \n        self.comp = comp\n        return X_reduced\n    \n    def scaler(self, df):\n        \n        _df = df.copy()\n            \n        if self.scaler_method == \"standard\":\n            return StandardScaler().fit_transform(_df)\n        elif self.scaler_method == \"minmax\":\n            return MinMaxScaler().fit_transform(_df)\n        elif self.scaler_method == None:\n            return _df.values\n        else:\n            raise ValueError(f\"Invalid scaler_method name\")\n        \n    def get_columns(self):\n        return [f'{self.method.upper()}_{_}' for _ in range(self.n_components)]\n    \n    def transform(self, df):\n        X = self.scaler(df)\n        X_reduced = self.comp.transform(X)\n        df_comp = pd.DataFrame(X_reduced, columns=[f'{self.method.upper()}_{_}' for _ in range(self.n_components)], index=df.index)\n        \n        return df_comp\n    @property\n        \n    def get_explained_variance_ratio(self):\n        \n        return np.sum(self.comp.explained_variance_ratio_)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:44.033034Z","iopub.execute_input":"2023-04-29T16:23:44.033465Z","iopub.status.idle":"2023-04-29T16:23:44.048544Z","shell.execute_reply.started":"2023-04-29T16:23:44.033423Z","shell.execute_reply":"2023-04-29T16:23:44.047386Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def decomp_concat(df):\n    global method\n    decomp = Decomp(n_components=3, method=method, scaler_method=None)\n    df_1 = decomp.dimension_reduction(df).reset_index(drop = True)\n    print(f\" the shape of df_1 is: {df_1.shape}\")\n    #df = df.reset_index(inplace = True)\n    df = pd.merge(df, df_1, left_index=True, right_index=True)\n    print(f\" the shape of df_2 2 is: {df.shape}\")\n    #df = pd.concat([train_X, df], axis=1)\n    #df = df.reset_index(inplace = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:44.054333Z","iopub.execute_input":"2023-04-29T16:23:44.054636Z","iopub.status.idle":"2023-04-29T16:23:44.061301Z","shell.execute_reply.started":"2023-04-29T16:23:44.054607Z","shell.execute_reply":"2023-04-29T16:23:44.060406Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"def decomp_concat(df):\n    global method\n    decomp = Decomp(n_components=9, method=method, scaler_method=None)\n    df_1 = decomp.dimension_reduction(df).reset_index(drop = True)\n    print(f\" the shape of df_1 is: {df_1.shape}\")\n    #df = df.reset_index(inplace = True)\n    df = pd.merge(df, df_1, left_index=True, right_index=True)\n    print(f\" the shape of df_2 2 is: {df.shape}\")\n    #df = pd.concat([train_X, df], axis=1)\n    #df = df.reset_index(inplace = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:44.062737Z","iopub.execute_input":"2023-04-29T16:23:44.063092Z","iopub.status.idle":"2023-04-29T16:23:44.073586Z","shell.execute_reply.started":"2023-04-29T16:23:44.063061Z","shell.execute_reply":"2023-04-29T16:23:44.072746Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"pca_func_trans = FunctionTransformer(decomp_concat)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:44.074969Z","iopub.execute_input":"2023-04-29T16:23:44.075616Z","iopub.status.idle":"2023-04-29T16:23:44.083512Z","shell.execute_reply.started":"2023-04-29T16:23:44.075570Z","shell.execute_reply":"2023-04-29T16:23:44.082748Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"pca_func_trans","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:44.084444Z","iopub.execute_input":"2023-04-29T16:23:44.084897Z","iopub.status.idle":"2023-04-29T16:23:44.095836Z","shell.execute_reply.started":"2023-04-29T16:23:44.084834Z","shell.execute_reply":"2023-04-29T16:23:44.094693Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"FunctionTransformer(func=<function decomp_concat at 0x79e6d1d868c0>)"},"metadata":{}}]},{"cell_type":"code","source":"def reset_index(dataframe):\n    dataframe = dataframe.reset_index(inplace = False)\n    return dataframe\n\nget_reset_index = FunctionTransformer(reset_index, validate=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:44.097069Z","iopub.execute_input":"2023-04-29T16:23:44.097361Z","iopub.status.idle":"2023-04-29T16:23:44.104415Z","shell.execute_reply.started":"2023-04-29T16:23:44.097331Z","shell.execute_reply":"2023-04-29T16:23:44.103668Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Set up a logistic regression pipeline\n#No feature engineering part )\n#pca_pipe = make_pipeline(PCA(n_components=2, random_state=0), LogisticRegression(random_state = seed, max_iter = 10000))\npca_pipe = make_pipeline( get_reset_index, pca_func_trans, LogisticRegression(random_state = seed, max_iter = 10000))\numap_pipe = make_pipeline(UMAP(n_components=3, random_state=0), LogisticRegression(random_state = seed, max_iter = 10000))","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:44.105922Z","iopub.execute_input":"2023-04-29T16:23:44.106223Z","iopub.status.idle":"2023-04-29T16:23:44.114728Z","shell.execute_reply.started":"2023-04-29T16:23:44.106194Z","shell.execute_reply":"2023-04-29T16:23:44.113618Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"pca_pipe","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:44.116008Z","iopub.execute_input":"2023-04-29T16:23:44.116697Z","iopub.status.idle":"2023-04-29T16:23:44.130852Z","shell.execute_reply.started":"2023-04-29T16:23:44.116652Z","shell.execute_reply":"2023-04-29T16:23:44.129698Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Pipeline(steps=[('functiontransformer-1',\n                 FunctionTransformer(func=<function reset_index at 0x79e6d1d30ef0>)),\n                ('functiontransformer-2',\n                 FunctionTransformer(func=<function decomp_concat at 0x79e6d1d868c0>)),\n                ('logisticregression',\n                 LogisticRegression(max_iter=10000, random_state=42))])"},"metadata":{}}]},{"cell_type":"code","source":"method = \"pca\"\ncross_val_pipe(pca_pipe)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:24:12.462496Z","iopub.execute_input":"2023-04-29T16:24:12.462939Z","iopub.status.idle":"2023-04-29T16:24:27.710602Z","shell.execute_reply.started":"2023-04-29T16:24:12.462897Z","shell.execute_reply":"2023-04-29T16:24:27.709281Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":" the shape of df_1 is: (639, 9)\n the shape of df_2 2 is: (639, 74)\n the shape of df_1 is: (639, 9)\n the shape of df_2 2 is: (639, 74)\n the shape of df_1 is: (320, 9)\n the shape of df_2 2 is: (320, 74)\n The val_score for 0 is 0.34010416666666665\n the shape of df_1 is: (639, 9)\n the shape of df_2 2 is: (639, 74)\n the shape of df_1 is: (639, 9)\n the shape of df_2 2 is: (639, 74)\n the shape of df_1 is: (320, 9)\n the shape of df_2 2 is: (320, 74)\n The val_score for 1 is 0.3260416666666667\n the shape of df_1 is: (640, 9)\n the shape of df_2 2 is: (640, 74)\n the shape of df_1 is: (640, 9)\n the shape of df_2 2 is: (640, 74)\n the shape of df_1 is: (319, 9)\n the shape of df_2 2 is: (319, 74)\n The val_score for 2 is 0.38610240334378265\nVal log_loss   : 2.20143 | Train log_loss   : 1.10697 | \nVal MAP@3 Score: 0.35075 | Train MAP@3 Score: 0.65128 | \n\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"([2.18195494131152, 2.2502745522666814, 2.1720717425052247],\n [0.34010416666666665, 0.3260416666666667, 0.38610240334378265])"},"metadata":{}}]},{"cell_type":"code","source":"def objective(trial):\n    param = {\n        \"loss_function\": trial.suggest_categorical(\"loss_function\", [\"RMSE\", \"MAE\"]),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e0),\n        \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-2, 1e0),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 2, 20),\n        \"one_hot_max_size\": trial.suggest_int(\"one_hot_max_size\", 2, 20),  \n    }\n    # Conditional Hyper-Parameters\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n    \n    #creating encoder and transforming prognosis\n    enc = LabelEncoder()\n    train_y = enc.fit_transform(train_y)\n    \n    #initiate prediction arrays and score lists\n    val_predictions = np.zeros((len(train_X), 11)) # Validation predictions are stored in a matrix with length of the number of trainijng samples and # of preds\n    train_predictions = np.zeros((len(train_X), 11)) # Same for the train predictions\n    train_logloss, val_logloss = [], [] #Store the results from the log_loss calc in a list\n    train_map3, val_map3 = [], [] #Store the results from the log_loss calc in a list\n    \n    #training model, predicting prognosis probability, and evaluating log loss\n    for fold, (train_idx, val_idx) in enumerate(cv.split(train_X, train_y)):\n                \n        model = CatBoostClassifier(**param)\n        model.fit(train_X.iloc[train_idx], train_y[train_idx])\n        \n        train_preds = model.predict_proba(train_X.iloc[train_idx])\n        val_preds = model.predict_proba(train_X.iloc[val_idx])\n                  \n        train_predictions[train_idx] += train_preds\n        val_predictions[val_idx] += val_preds\n        \n        train_score = log_loss(train_y[train_idx], train_preds)\n        val_score = log_loss(train_y[val_idx], val_preds)\n        \n        train_logloss.append(train_score)\n        val_logloss.append(val_score)\n        \n        #select three most probable prognosis based on train dataset prediction\n        train_index = np.argsort(-train_preds)[:,:3] #return index of three most probable prognosis\n        \n        #select three most probable prognosis based on validation dataset prediction\n        val_index = np.argsort(-val_preds)[:,:3]\n    \n        #calculate map@3\n        train_score = mapk(train_y[train_idx].reshape(-1, 1), train_index, 3)\n        val_score = mapk(train_y[val_idx].reshape(-1, 1), val_index, 3)\n        print(f\" The val_score for {fold} is {val_score}\")\n        \n        train_map3.append(train_score)\n        val_map3.append(val_score)\n    \n    print(f'Val log_loss   : {np.mean(val_logloss):.5f} | Train log_loss   : {np.mean(train_logloss):.5f} | {label}')\n    print(f'Val MAP@3 Score: {np.mean(val_map3):.5f} | Train MAP@3 Score: {np.mean(train_map3):.5f} | {label}\\n')\n    \n    return val_logloss, val_map3\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:24:54.521083Z","iopub.execute_input":"2023-04-29T16:24:54.521547Z","iopub.status.idle":"2023-04-29T16:24:54.541206Z","shell.execute_reply.started":"2023-04-29T16:24:54.521506Z","shell.execute_reply":"2023-04-29T16:24:54.540200Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=3, timeout=600)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:25:09.358750Z","iopub.execute_input":"2023-04-29T16:25:09.359175Z","iopub.status.idle":"2023-04-29T16:25:09.392842Z","shell.execute_reply.started":"2023-04-29T16:25:09.359137Z","shell.execute_reply":"2023-04-29T16:25:09.391339Z"},"trusted":true},"execution_count":23,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_243/2906504016.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of finished trials: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'optuna' is not defined"],"ename":"NameError","evalue":"name 'optuna' is not defined","output_type":"error"}]},{"cell_type":"code","source":"#Create data to plot the learning rate \nfor i in range(5,11,1):\n    X = train.copy().sample(frac = i/10)\n    y = X.pop(\"prognosis\")\n    \n    #creating encoder and transforming prognosis\n    enc = LabelEncoder()\n    y = enc.fit_transform(y)\n    \n    #initiate prediction arrays and score lists\n    val_predictions = np.zeros((len(X), 11)) # Validation predictions are stored in a matrix with length of the number of trainijng samples and # of preds\n    train_predictions = np.zeros((len(X), 11)) # Same for the train predictions\n    train_logloss, val_logloss = [], [] #Store the results from the log_loss calc in a list\n    train_map3, val_map3 = [], [] #Store the results from the log_loss calc in a list\n    \n    #training model, predicting prognosis probability, and evaluating log loss\n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n                \n        model.fit(X.iloc[train_idx], y[train_idx])\n        \n        train_preds = model.predict_proba(X.iloc[train_idx])\n        val_preds = model.predict_proba(X.iloc[val_idx])\n                  \n        train_predictions[train_idx] += train_preds\n        val_predictions[val_idx] += val_preds\n        \n        train_score = log_loss(y[train_idx], train_preds)\n        val_score = log_loss(y[val_idx], val_preds)\n        \n        train_logloss.append(train_score)\n        val_logloss.append(val_score)\n        \n        #select three most probable prognosis based on train dataset prediction\n        train_index = np.argsort(-train_preds)[:,:3] #return index of three most probable prognosis\n        \n        #select three most probable prognosis based on validation dataset prediction\n        val_index = np.argsort(-val_preds)[:,:3]\n    \n        #calculate map@3\n        train_score = mapk(y[train_idx].reshape(-1, 1), train_index, 3)\n        val_score = mapk(y[val_idx].reshape(-1, 1), val_index, 3)\n        \n        train_map3.append(train_score)\n        val_map3.append(val_score)\n    \n    print(f'Val log_loss  for {i} : {np.mean(val_logloss):.5f} | Train log_loss   : {np.mean(train_logloss):.5f}')\n    print(f'Val MAP@3 Score for {i}: {np.mean(val_map3):.5f} | Train MAP@3 Score: {np.mean(train_map3):.5f}' )","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:44.684222Z","iopub.status.idle":"2023-04-29T16:23:44.684622Z","shell.execute_reply.started":"2023-04-29T16:23:44.684433Z","shell.execute_reply":"2023-04-29T16:23:44.684453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create pipeline where the best estimator type can selected","metadata":{}},{"cell_type":"code","source":"pca_pipe = make_pipeline(PCA(n_components=2, random_state=0), model)\n#pca_pipe = make_pipeline(PCA(n_components=2, random_state=0), LogisticRegression(random_state = seed, max_iter = 10000))\n#pca_pipe = make_pipeline( get_reset_index, pca_func_trans, LogisticRegression(random_state = seed, max_iter = 10000))\n#umap_pipe = make_pipeline(UMAP(n_components=3, random_state=0), LogisticRegression(random_state = seed, max_iter = 10000))","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:44.686420Z","iopub.status.idle":"2023-04-29T16:23:44.686869Z","shell.execute_reply.started":"2023-04-29T16:23:44.686629Z","shell.execute_reply":"2023-04-29T16:23:44.686651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logloss_list, map3_list = pd.DataFrame(), pd.DataFrame()\n\nfor (label, model) in models: #Aha, the label in the print function comes from here, as the models had been setuo as a list of tuples\n    model_pipeline = make_pipeline(PCA(n_components=2, random_state=0), model)\n    (logloss_list[label], map3_list[label]) = cross_val_pipe(model_pipeline, label = label)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:44.689062Z","iopub.status.idle":"2023-04-29T16:23:44.689443Z","shell.execute_reply.started":"2023-04-29T16:23:44.689255Z","shell.execute_reply":"2023-04-29T16:23:44.689275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logloss_list_v1, map4_list = pd.DataFrame(), pd.DataFrame()\n\nfor (label, model) in transformers: #Aha, the label in the print function comes from here, as the models had been setuo as a list of tuples\n    (logloss_list_v1[label], map4_list[label]) = cross_val_pipe(model, label = label)","metadata":{"execution":{"iopub.status.busy":"2023-04-29T16:23:44.690440Z","iopub.status.idle":"2023-04-29T16:23:44.690885Z","shell.execute_reply.started":"2023-04-29T16:23:44.690651Z","shell.execute_reply":"2023-04-29T16:23:44.690673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}