{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, roc_curve, log_loss, make_scorer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, FunctionTransformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.decomposition import PCA, NMF\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.manifold import TSNE\nfrom umap import UMAP\nfrom scipy.cluster.hierarchy import dendrogram, ward\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\n\nsns.set_theme(style = 'white', palette = 'viridis')\npal = sns.color_palette('viridis')\npd.set_option('display.max_rows', 100)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:12:22.915683Z","iopub.execute_input":"2023-04-30T17:12:22.916476Z","iopub.status.idle":"2023-04-30T17:12:49.622293Z","shell.execute_reply.started":"2023-04-30T17:12:22.916404Z","shell.execute_reply":"2023-04-30T17:12:49.621306Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## How to define the different states of the data","metadata":{}},{"cell_type":"code","source":"#Variable for the imported training data after cleaning up - \"train\"\n#Variable for the feature data set - \"train_X\"\n#Variable for the target series- \"train_y\"\n#Variable for the  target colum - \"target\"\n#Variable for the imported test data set - \"test\"\n#Variable for the transformed test data set - \"test_trans\"\n#Variable for the training data set in the cross validation loop - \"X_train\"\n#Variable for the validation data set in the cross validation loop - \"X_val\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:12:49.624377Z","iopub.execute_input":"2023-04-30T17:12:49.624755Z","iopub.status.idle":"2023-04-30T17:12:49.629528Z","shell.execute_reply.started":"2023-04-30T17:12:49.624723Z","shell.execute_reply":"2023-04-30T17:12:49.628300Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Downloading data and getting into a feature matrix and labels vector","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(r'../input/playground-series-s3e13/train.csv')\ntest_1 = pd.read_csv(r'../input/playground-series-s3e13/test.csv')\norig_train = pd.read_csv(r'../input/vector-borne-disease-prediction/trainn.csv')\n\ntrain.drop('id', axis = 1, inplace = True)\ntest = test_1.drop('id', axis = 1)\n\ntarget = 'prognosis'","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:12:49.631408Z","iopub.execute_input":"2023-04-30T17:12:49.631827Z","iopub.status.idle":"2023-04-30T17:12:49.695651Z","shell.execute_reply.started":"2023-04-30T17:12:49.631796Z","shell.execute_reply":"2023-04-30T17:12:49.694491Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train, orig_train])\nprint(f'There are {train.duplicated(subset = list(train)[0:-1]).value_counts()[0]} non-duplicate values out of {train.count()[0]} rows in original train dataset')","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:12:49.696949Z","iopub.execute_input":"2023-04-30T17:12:49.697286Z","iopub.status.idle":"2023-04-30T17:12:49.725154Z","shell.execute_reply.started":"2023-04-30T17:12:49.697253Z","shell.execute_reply":"2023-04-30T17:12:49.724199Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"There are 959 non-duplicate values out of 959 rows in original train dataset\n","output_type":"stream"}]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:12:49.727485Z","iopub.execute_input":"2023-04-30T17:12:49.727822Z","iopub.status.idle":"2023-04-30T17:12:49.735364Z","shell.execute_reply.started":"2023-04-30T17:12:49.727788Z","shell.execute_reply":"2023-04-30T17:12:49.734347Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(959, 65)"},"metadata":{}}]},{"cell_type":"code","source":"train_X = train.drop(target, axis = 1).copy()\ntrain_y = train[target]\n\n#Fix the issues that the original data set used \"-\"to separate the words\ntrain_y = [prognosis.replace(' ', '_') for prognosis in train_y]\ntrain_y = np.array(train_y)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:12:49.736528Z","iopub.execute_input":"2023-04-30T17:12:49.736867Z","iopub.status.idle":"2023-04-30T17:12:49.748626Z","shell.execute_reply.started":"2023-04-30T17:12:49.736837Z","shell.execute_reply":"2023-04-30T17:12:49.747646Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_X.shape, train_y.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:12:49.750521Z","iopub.execute_input":"2023-04-30T17:12:49.751298Z","iopub.status.idle":"2023-04-30T17:12:49.762241Z","shell.execute_reply.started":"2023-04-30T17:12:49.751253Z","shell.execute_reply":"2023-04-30T17:12:49.761114Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"((959, 64), (959,))"},"metadata":{}}]},{"cell_type":"markdown","source":"### Cross-validation strategy","metadata":{}},{"cell_type":"code","source":"#Cross validation strategy\nseed = 42\nsplits = 5\n#cv = RepeatedStratifiedKFold(n_splits = splits, n_repeats = 5, random_state = seed)\ncv = StratifiedKFold(n_splits = splits, random_state = seed, shuffle = True)\n\nnp.random.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T20:09:18.462593Z","iopub.execute_input":"2023-04-30T20:09:18.463655Z","iopub.status.idle":"2023-04-30T20:09:18.469684Z","shell.execute_reply.started":"2023-04-30T20:09:18.463613Z","shell.execute_reply":"2023-04-30T20:09:18.468407Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation metric","metadata":{}},{"cell_type":"code","source":"def apk(actual, predicted, k=10):\n    \n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i,p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    if not actual:\n        return 0.0\n\n    return score / min(len(actual), k)\n\n\ndef mapk(actual, predicted, k=10):\n    \n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:12:49.772781Z","iopub.execute_input":"2023-04-30T17:12:49.773088Z","iopub.status.idle":"2023-04-30T17:12:49.783396Z","shell.execute_reply.started":"2023-04-30T17:12:49.773057Z","shell.execute_reply":"2023-04-30T17:12:49.782602Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def cross_val_pipe(model, train_X = train_X, train_y = train_y, target ='prognosis', cv = cv, label = ''):\n    \n    \n    #creating encoder and transforming prognosis\n    enc = LabelEncoder()\n    train_y = enc.fit_transform(train_y)\n    \n    #initiate prediction arrays and score lists\n    val_predictions = np.zeros((len(train_X), 11)) # Validation predictions are stored in a matrix with length of the number of trainijng samples and # of preds\n    train_predictions = np.zeros((len(train_X), 11)) # Same for the train predictions\n    train_logloss, val_logloss = [], [] #Store the results from the log_loss calc in a list\n    train_map3, val_map3 = [], [] #Store the results from the log_loss calc in a list\n    \n    #training model, predicting prognosis probability, and evaluating log loss\n    for fold, (train_idx, val_idx) in enumerate(cv.split(train_X, train_y)):\n                \n        model.fit(train_X.iloc[train_idx], train_y[train_idx])\n        \n        train_preds = model.predict_proba(train_X.iloc[train_idx])\n        val_preds = model.predict_proba(train_X.iloc[val_idx])\n                  \n        train_predictions[train_idx] += train_preds\n        val_predictions[val_idx] += val_preds\n        \n        train_score = log_loss(train_y[train_idx], train_preds)\n        val_score = log_loss(train_y[val_idx], val_preds)\n        \n        train_logloss.append(train_score)\n        val_logloss.append(val_score)\n        \n        #select three most probable prognosis based on train dataset prediction\n        train_index = np.argsort(-train_preds)[:,:3] #return index of three most probable prognosis\n        \n        #select three most probable prognosis based on validation dataset prediction\n        val_index = np.argsort(-val_preds)[:,:3]\n    \n        #calculate map@3\n        train_score = mapk(train_y[train_idx].reshape(-1, 1), train_index, 3)\n        val_score = mapk(train_y[val_idx].reshape(-1, 1), val_index, 3)\n        print(f\" The val_score for {fold} is {val_score}\")\n        \n        train_map3.append(train_score)\n        val_map3.append(val_score)\n    \n    print(f'Val log_loss   : {np.mean(val_logloss):.5f} | Train log_loss   : {np.mean(train_logloss):.5f} | {label}')\n    print(f'Val MAP@3 Score: {np.mean(val_map3):.5f} | Train MAP@3 Score: {np.mean(train_map3):.5f} | {label}\\n')\n    \n    return val_logloss, val_map3","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:12:49.786325Z","iopub.execute_input":"2023-04-30T17:12:49.787163Z","iopub.status.idle":"2023-04-30T17:12:49.800346Z","shell.execute_reply.started":"2023-04-30T17:12:49.787117Z","shell.execute_reply":"2023-04-30T17:12:49.799481Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Models and Parameters for Multi-class Classification","metadata":{}},{"cell_type":"code","source":"# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","metadata":{"execution":{"iopub.status.busy":"2023-04-30T20:31:14.366925Z","iopub.execute_input":"2023-04-30T20:31:14.368147Z","iopub.status.idle":"2023-04-30T20:31:14.377195Z","shell.execute_reply.started":"2023-04-30T20:31:14.368084Z","shell.execute_reply":"2023-04-30T20:31:14.375526Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"#Classification models\n\n#List of tuples\nmodels = [\n    ('log', LogisticRegression(random_state = seed, max_iter = 10000)),\n    #('svc', SVC(random_state = seed, probability = True)),\n    #('lda', LinearDiscriminantAnalysis()),\n    #('qda', QuadraticDiscriminantAnalysis()),\n    #('gauss', GaussianProcessClassifier(random_state = seed)),\n    #('et', ExtraTreesClassifier(random_state = seed)),\n    #('rf', RandomForestClassifier(random_state = seed)),\n    #('xgb', XGBClassifier(random_state = seed, objective = 'multi:softprob', eval_metric = 'map@3')),\n    ('lgb', LGBMClassifier(random_state = seed, objective = 'softmax', metric = 'softmax')),\n    #('dart', LGBMClassifier(random_state = seed, objective = 'softmax', metric = 'softmax', boosting_type = 'dart')),\n    #('cb', CatBoostClassifier(random_state = seed, objective = 'MultiClass', verbose = 0)),\n    #('gb', GradientBoostingClassifier(random_state = seed)),\n    #('hgb', HistGradientBoostingClassifier(random_state = seed)),\n    #('ada', AdaBoostClassifier(random_state = seed)),\n    #('knn', KNeighborsClassifier())\n]","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:12:49.801704Z","iopub.execute_input":"2023-04-30T17:12:49.801979Z","iopub.status.idle":"2023-04-30T17:12:49.815084Z","shell.execute_reply.started":"2023-04-30T17:12:49.801953Z","shell.execute_reply":"2023-04-30T17:12:49.814222Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Dimension reduction - TNSE, PCA and other clustering based algorythms","metadata":{}},{"cell_type":"code","source":"class Decomp:\n    def __init__(self, n_components, method=\"pca\", scaler_method='standard'):\n        self.n_components = n_components\n        self.method = method\n        self.scaler_method = scaler_method\n        \n    def dimension_reduction(self, df):\n            \n        X_reduced = self.dimension_method(df)\n        df_comp = pd.DataFrame(X_reduced, columns=[f'{self.method.upper()}_{_}' for _ in range(self.n_components)], index=df.index)\n        \n        return df_comp\n    \n    def dimension_method(self, df):\n        X = self.scaler(df)\n        if self.method == \"pca\":\n            comp = PCA(n_components=self.n_components, random_state=0)\n            X_reduced = comp.fit_transform(X)\n        elif self.method == \"nmf\":\n            comp = NMF(n_components=self.n_components, random_state=0)\n            X_reduced = comp.fit_transform(X)\n        elif self.method == \"umap\":\n            comp = UMAP(n_components=self.n_components, random_state=0)\n            X_reduced = comp.fit_transform(X)\n        elif self.method == \"tsne\":\n            comp = TSNE(n_components=self.n_components, random_state=0) # Recommend n_components=2\n            X_reduced = comp.fit_transform(X)\n        else:\n            raise ValueError(f\"Invalid method name: {method}\")\n        \n        self.comp = comp\n        return X_reduced\n    \n    def scaler(self, df):\n        \n        _df = df.copy()\n            \n        if self.scaler_method == \"standard\":\n            return StandardScaler().fit_transform(_df)\n        elif self.scaler_method == \"minmax\":\n            return MinMaxScaler().fit_transform(_df)\n        elif self.scaler_method == None:\n            return _df.values\n        else:\n            raise ValueError(f\"Invalid scaler_method name\")\n        \n    def get_columns(self):\n        return [f'{self.method.upper()}_{_}' for _ in range(self.n_components)]\n    \n    def transform(self, df):\n        X = self.scaler(df)\n        X_reduced = self.comp.transform(X)\n        df_comp = pd.DataFrame(X_reduced, columns=[f'{self.method.upper()}_{_}' for _ in range(self.n_components)], index=df.index)\n        \n        return df_comp\n    @property\n        \n    def get_explained_variance_ratio(self):\n        \n        return np.sum(self.comp.explained_variance_ratio_)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:12:49.816620Z","iopub.execute_input":"2023-04-30T17:12:49.817217Z","iopub.status.idle":"2023-04-30T17:12:49.831319Z","shell.execute_reply.started":"2023-04-30T17:12:49.817184Z","shell.execute_reply":"2023-04-30T17:12:49.830346Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def decomp_concat(df):\n    global method\n    decomp = Decomp(n_components=3, method=method, scaler_method=None)\n    df_1 = decomp.dimension_reduction(df).reset_index(drop = True)\n    print(f\" the shape of df_1 is: {df_1.shape}\")\n    #df = df.reset_index(inplace = True)\n    df = pd.merge(df, df_1, left_index=True, right_index=True)\n    print(f\" the shape of df_2 2 is: {df.shape}\")\n    #df = pd.concat([train_X, df], axis=1)\n    #df = df.reset_index(inplace = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:12:49.832833Z","iopub.execute_input":"2023-04-30T17:12:49.833151Z","iopub.status.idle":"2023-04-30T17:12:49.846378Z","shell.execute_reply.started":"2023-04-30T17:12:49.833107Z","shell.execute_reply":"2023-04-30T17:12:49.845497Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def decomp_concat(df):\n    global method\n    decomp = Decomp(n_components=9, method=method, scaler_method=None)\n    df_1 = decomp.dimension_reduction(df).reset_index(drop = True)\n    print(f\" the shape of df_1 is: {df_1.shape}\")\n    #df = df.reset_index(inplace = True)\n    df = pd.merge(df, df_1, left_index=True, right_index=True)\n    print(f\" the shape of df_2 2 is: {df.shape}\")\n    #df = pd.concat([train_X, df], axis=1)\n    #df = df.reset_index(inplace = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:13:22.777968Z","iopub.execute_input":"2023-04-30T17:13:22.778643Z","iopub.status.idle":"2023-04-30T17:13:22.785868Z","shell.execute_reply.started":"2023-04-30T17:13:22.778596Z","shell.execute_reply":"2023-04-30T17:13:22.784687Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Pipeline","metadata":{}},{"cell_type":"code","source":"# Function that is sometimes needed especially with concat data\ndef reset_index(dataframe):\n    dataframe = dataframe.reset_index(inplace = False)\n    return dataframe\n\nget_reset_index = FunctionTransformer(reset_index, validate=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:19:40.182207Z","iopub.execute_input":"2023-04-30T17:19:40.182718Z","iopub.status.idle":"2023-04-30T17:19:40.189894Z","shell.execute_reply.started":"2023-04-30T17:19:40.182673Z","shell.execute_reply":"2023-04-30T17:19:40.188405Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"pca_func_trans = FunctionTransformer(decomp_concat)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:13:22.787653Z","iopub.execute_input":"2023-04-30T17:13:22.788037Z","iopub.status.idle":"2023-04-30T17:13:22.795703Z","shell.execute_reply.started":"2023-04-30T17:13:22.788006Z","shell.execute_reply":"2023-04-30T17:13:22.794760Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"pca_func_trans","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:13:22.796771Z","iopub.execute_input":"2023-04-30T17:13:22.797523Z","iopub.status.idle":"2023-04-30T17:13:22.815505Z","shell.execute_reply.started":"2023-04-30T17:13:22.797487Z","shell.execute_reply":"2023-04-30T17:13:22.814253Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"FunctionTransformer(func=<function decomp_concat at 0x72ab3c8ebef0>)"},"metadata":{}}]},{"cell_type":"code","source":"#Set up a logistic regression pipeline\n#No feature engineering part )\n#pca_pipe = make_pipeline(PCA(n_components=2, random_state=0), LogisticRegression(random_state = seed, max_iter = 10000))\npca_pipe = make_pipeline( get_reset_index, pca_func_trans, LogisticRegression(random_state = seed, max_iter = 10000))\numap_pipe = make_pipeline(UMAP(n_components=3, random_state=0), LogisticRegression(random_state = seed, max_iter = 10000))","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:13:22.825889Z","iopub.execute_input":"2023-04-30T17:13:22.826569Z","iopub.status.idle":"2023-04-30T17:13:22.834763Z","shell.execute_reply.started":"2023-04-30T17:13:22.826519Z","shell.execute_reply":"2023-04-30T17:13:22.833838Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"pca_pipe","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:13:22.836232Z","iopub.execute_input":"2023-04-30T17:13:22.836545Z","iopub.status.idle":"2023-04-30T17:13:22.849321Z","shell.execute_reply.started":"2023-04-30T17:13:22.836516Z","shell.execute_reply":"2023-04-30T17:13:22.848499Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"Pipeline(steps=[('functiontransformer-1',\n                 FunctionTransformer(func=<function reset_index at 0x72ab3cbd0320>)),\n                ('functiontransformer-2',\n                 FunctionTransformer(func=<function decomp_concat at 0x72ab3c8ebef0>)),\n                ('logisticregression',\n                 LogisticRegression(max_iter=10000, random_state=42))])"},"metadata":{}}]},{"cell_type":"code","source":"cross_val_pipe(pca_pipe)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T17:13:22.851016Z","iopub.execute_input":"2023-04-30T17:13:22.851298Z","iopub.status.idle":"2023-04-30T17:14:22.878168Z","shell.execute_reply.started":"2023-04-30T17:13:22.851270Z","shell.execute_reply":"2023-04-30T17:14:22.876269Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":" the shape of df_1 is: (639, 9)\n the shape of df_2 2 is: (639, 74)\n the shape of df_1 is: (639, 9)\n the shape of df_2 2 is: (639, 74)\n the shape of df_1 is: (320, 9)\n the shape of df_2 2 is: (320, 74)\n The val_score for 0 is 0.3234375\n the shape of df_1 is: (639, 9)\n the shape of df_2 2 is: (639, 74)\n the shape of df_1 is: (639, 9)\n the shape of df_2 2 is: (639, 74)\n the shape of df_1 is: (320, 9)\n the shape of df_2 2 is: (320, 74)\n The val_score for 1 is 0.29166666666666663\n the shape of df_1 is: (640, 9)\n the shape of df_2 2 is: (640, 74)\n the shape of df_1 is: (640, 9)\n the shape of df_2 2 is: (640, 74)\n the shape of df_1 is: (319, 9)\n the shape of df_2 2 is: (319, 74)\n The val_score for 2 is 0.2507836990595611\nVal log_loss   : 3.67350 | Train log_loss   : 1.00557 | \nVal MAP@3 Score: 0.28863 | Train MAP@3 Score: 0.68039 | \n\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"([2.9812251379993095, 3.914486132290827, 4.124775095667349],\n [0.3234375, 0.29166666666666663, 0.2507836990595611])"},"metadata":{}}]},{"cell_type":"markdown","source":"# Create pipeline where the best estimator type can selected","metadata":{}},{"cell_type":"code","source":"pca_pipe = make_pipeline(PCA(n_components=2, random_state=0), model)\n#pca_pipe = make_pipeline(PCA(n_components=2, random_state=0), LogisticRegression(random_state = seed, max_iter = 10000))\n#pca_pipe = make_pipeline( get_reset_index, pca_func_trans, LogisticRegression(random_state = seed, max_iter = 10000))\n#umap_pipe = make_pipeline(UMAP(n_components=3, random_state=0), LogisticRegression(random_state = seed, max_iter = 10000))","metadata":{"execution":{"iopub.status.busy":"2023-04-29T09:36:04.871024Z","iopub.execute_input":"2023-04-29T09:36:04.871563Z","iopub.status.idle":"2023-04-29T09:36:04.879776Z","shell.execute_reply.started":"2023-04-29T09:36:04.871522Z","shell.execute_reply":"2023-04-29T09:36:04.877991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create the cross-validation pipeline that stores the predictions","metadata":{}},{"cell_type":"markdown","source":"### Loop for fitting the models and making predictions","metadata":{}},{"cell_type":"code","source":"\ndef cross_val_pipe(model, train_X = train_X, train_y = train_y, target ='prognosis', cv = cv, label = ''):\n    \n    \n    #creating encoder and transforming prognosis\n    enc = LabelEncoder()\n    train_y = enc.fit_transform(train_y)\n    \n    #initiate prediction arrays and score lists\n    val_predictions = np.zeros((len(train_X), 11)) # Validation predictions are stored in a matrix with length of the number of trainijng samples and # of preds\n    train_predictions = np.zeros((len(train_X), 11)) # Same for the train predictions\n    train_logloss, val_logloss = [], [] #Store the results from the log_loss calc in a list\n    train_map3, val_map3 = [], [] #Store the results from the log_loss calc in a list\n    \n    #training model, predicting prognosis probability, and evaluating log loss\n    for fold, (train_idx, val_idx) in enumerate(cv.split(train_X, train_y)):\n                \n        model.fit(train_X.iloc[train_idx], train_y[train_idx])\n        \n        train_preds = model.predict_proba(train_X.iloc[train_idx])\n        val_preds = model.predict_proba(train_X.iloc[val_idx])\n                  \n        train_predictions[train_idx] += train_preds\n        val_predictions[val_idx] += val_preds\n        \n        train_score = log_loss(train_y[train_idx], train_preds)\n        val_score = log_loss(train_y[val_idx], val_preds)\n        \n        train_logloss.append(train_score)\n        val_logloss.append(val_score)\n        \n        #select three most probable prognosis based on train dataset prediction\n        train_index = np.argsort(-train_preds)[:,:3] #return index of three most probable prognosis\n        \n        #select three most probable prognosis based on validation dataset prediction\n        val_index = np.argsort(-val_preds)[:,:3]\n    \n        #calculate map@3\n        train_score = mapk(train_y[train_idx].reshape(-1, 1), train_index, 3)\n        val_score = mapk(train_y[val_idx].reshape(-1, 1), val_index, 3)\n        print(f\" The val_score for {fold} is {val_score}\")\n        \n        train_map3.append(train_score)\n        val_map3.append(val_score)\n        val_predictions.append(val_predictions)\n    \n    print(f'Val log_loss   : {np.mean(val_logloss):.5f} | Train log_loss   : {np.mean(train_logloss):.5f} | {label}')\n    print(f'Val MAP@3 Score: {np.mean(val_map3):.5f} | Train MAP@3 Score: {np.mean(train_map3):.5f} | {label}\\n')\n    \n    return val_logloss, val_map3, val_predictions","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:02:24.272445Z","iopub.execute_input":"2023-04-30T18:02:24.273480Z","iopub.status.idle":"2023-04-30T18:02:24.283537Z","shell.execute_reply.started":"2023-04-30T18:02:24.273400Z","shell.execute_reply":"2023-04-30T18:02:24.282779Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### Class-basd approach","metadata":{}},{"cell_type":"code","source":"# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T19:54:05.502354Z","iopub.execute_input":"2023-04-30T19:54:05.503282Z","iopub.status.idle":"2023-04-30T19:54:05.510379Z","shell.execute_reply.started":"2023-04-30T19:54:05.503242Z","shell.execute_reply":"2023-04-30T19:54:05.509488Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier(random_state = 49, **rf_params)\net = ExtraTreesClassifier(random_state = 49, **et_params)\ngb = ExtraTreesClassifier(random_state = 49, **gb_params)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T20:00:07.873633Z","iopub.execute_input":"2023-04-30T20:00:07.874076Z","iopub.status.idle":"2023-04-30T20:00:07.880071Z","shell.execute_reply.started":"2023-04-30T20:00:07.874038Z","shell.execute_reply":"2023-04-30T20:00:07.878892Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"#Cross-validation applications\n# Model selection\n# Determining the error metric for the pipeline\n# Predicting the labels for the training data sets - Only use them from cross-validation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enc = LabelEncoder()\ntrain_y = enc.fit_transform(train_y)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T20:56:08.750222Z","iopub.execute_input":"2023-04-30T20:56:08.750849Z","iopub.status.idle":"2023-04-30T20:56:08.756890Z","shell.execute_reply.started":"2023-04-30T20:56:08.750788Z","shell.execute_reply":"2023-04-30T20:56:08.755545Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"val_predictions = np.zeros((len(train_X), 11))","metadata":{"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"},"metadata":{}}]},{"cell_type":"code","source":"model.fit(train_X.iloc[1:200], train_y[1:200])","metadata":{"execution":{"iopub.status.busy":"2023-04-30T21:04:42.852880Z","iopub.execute_input":"2023-04-30T21:04:42.853286Z","iopub.status.idle":"2023-04-30T21:04:42.925747Z","shell.execute_reply.started":"2023-04-30T21:04:42.853253Z","shell.execute_reply":"2023-04-30T21:04:42.924395Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"LogisticRegression(max_iter=10000, random_state=42)"},"metadata":{}}]},{"cell_type":"code","source":"val_preds = model.predict_proba(train_X.iloc[201:300])","metadata":{"execution":{"iopub.status.busy":"2023-04-30T21:11:40.289664Z","iopub.execute_input":"2023-04-30T21:11:40.290787Z","iopub.status.idle":"2023-04-30T21:11:40.299133Z","shell.execute_reply.started":"2023-04-30T21:11:40.290729Z","shell.execute_reply":"2023-04-30T21:11:40.297932Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"val_preds[1:5].shape","metadata":{"execution":{"iopub.status.busy":"2023-04-30T21:13:03.524811Z","iopub.execute_input":"2023-04-30T21:13:03.525228Z","iopub.status.idle":"2023-04-30T21:13:03.533041Z","shell.execute_reply.started":"2023-04-30T21:13:03.525188Z","shell.execute_reply":"2023-04-30T21:13:03.531770Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"(4, 11)"},"metadata":{}}]},{"cell_type":"code","source":"val_predictions[1:10]","metadata":{"execution":{"iopub.status.busy":"2023-04-30T21:15:48.437486Z","iopub.execute_input":"2023-04-30T21:15:48.437954Z","iopub.status.idle":"2023-04-30T21:15:48.446848Z","shell.execute_reply.started":"2023-04-30T21:15:48.437912Z","shell.execute_reply":"2023-04-30T21:15:48.445861Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"},"metadata":{}}]},{"cell_type":"code","source":"def cross_val_pred(model, train_X = train_X, train_y = train_y, target ='prognosis', cv = cv, label = ''):\n    \n    \n    #initiate prediction arrays and score lists\n    val_predictions = np.zeros((len(train_X), 3)) # Validation predictions are stored in a matrix with length of the number of trainijng samples and # of preds\n    \n    #training model, predicting prognosis probability, and evaluating log loss\n    for fold, (train_idx, val_idx) in enumerate(cv.split(train_X, train_y)):\n                \n        model.fit(train_X.iloc[train_idx], train_y[train_idx])\n        \n        \n        val_preds = model.predict_proba(train_X.iloc[val_idx])\n                  \n       \n        val_predictions[val_idx] = val_preds\n        \n        train_score = log_loss(train_y[train_idx], train_preds)\n        val_score = log_loss(train_y[val_idx], val_preds)\n        \n        train_logloss.append(train_score)\n        val_logloss.append(val_score)\n        \n        #select three most probable prognosis based on train dataset prediction\n        train_index = np.argsort(-train_preds)[:,:3] #return index of three most probable prognosis\n        \n        #select three most probable prognosis based on validation dataset prediction\n        val_index = np.argsort(-val_preds)[:,:3]\n    \n        #calculate map@3\n        train_score = mapk(train_y[train_idx].reshape(-1, 1), train_index, 3)\n        val_score = mapk(train_y[val_idx].reshape(-1, 1), val_index, 3)\n        print(f\" The val_score for {fold} is {val_score}\")\n        \n        train_map3.append(train_score)\n        val_map3.append(val_score)\n    \n    print(f'Val log_loss   : {np.mean(val_logloss):.5f} | Train log_loss   : {np.mean(train_logloss):.5f} | {label}')\n    print(f'Val MAP@3 Score: {np.mean(val_map3):.5f} | Train MAP@3 Score: {np.mean(train_map3):.5f} | {label}\\n')\n    \n    return val_logloss, val_map3","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Comment - Determine the best hyperparamter und feature engineering strategy separately\n\n#Input - list of pipelines, the training data, the cross validation strategy\n#Output dataframe with the predictions for each classifier\n\n#Get empty df\n\npred_df = pd.DataFrame()\n\nfor (label, pipeline) in pipelines:\n    pred_df[label] = cross_val_pipe(pipeline, label = label)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-30T20:47:41.753476Z","iopub.execute_input":"2023-04-30T20:47:41.754768Z","iopub.status.idle":"2023-04-30T20:47:41.760953Z","shell.execute_reply.started":"2023-04-30T20:47:41.754715Z","shell.execute_reply":"2023-04-30T20:47:41.759492Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"# Workflow","metadata":{}},{"cell_type":"code","source":"# 1.) Create a function to load the data and do the general feature engineering steps\n# 2.) Define the cross-validation strategy\n# 3.) Define the loss function and the metric\n# 4.) Set up the feature engineering pipeline for each classifier\n# 5.) Perform hyperparameter optimization\n# 6.) Fit, predict, score ","metadata":{},"execution_count":null,"outputs":[]}]}