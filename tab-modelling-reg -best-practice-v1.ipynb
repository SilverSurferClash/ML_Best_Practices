{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\n#Import data manipulation libaries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n","metadata":{"papermill":{"duration":0.026142,"end_time":"2023-03-29T16:43:56.293495","exception":false,"start_time":"2023-03-29T16:43:56.267353","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-30T19:58:57.979835Z","iopub.execute_input":"2023-03-30T19:58:57.980416Z","iopub.status.idle":"2023-03-30T19:58:57.991279Z","shell.execute_reply.started":"2023-03-30T19:58:57.980380Z","shell.execute_reply":"2023-03-30T19:58:57.990031Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"#### First data loading strategy","metadata":{"papermill":{"duration":0.002334,"end_time":"2023-03-29T16:43:56.298908","exception":false,"start_time":"2023-03-29T16:43:56.296574","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_import = pd.read_csv(\"/kaggle/input/playground-series-s3e6/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/playground-series-s3e6/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/playground-series-s3e6/sample_submission.csv\")","metadata":{"papermill":{"duration":2.019571,"end_time":"2023-03-29T16:43:58.321008","exception":false,"start_time":"2023-03-29T16:43:56.301437","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-30T19:58:57.994028Z","iopub.execute_input":"2023-03-30T19:58:57.994987Z","iopub.status.idle":"2023-03-30T19:58:58.125189Z","shell.execute_reply.started":"2023-03-30T19:58:57.994945Z","shell.execute_reply":"2023-03-30T19:58:58.124161Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"#### Second data loading strategy\nLoading another training data set\nAdding a flag for the adversial cross-validation","metadata":{"papermill":{"duration":0.002227,"end_time":"2023-03-29T16:43:58.326112","exception":false,"start_time":"2023-03-29T16:43:58.323885","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/playground-series-s3e6/test.csv')\ntest.drop(\"id\",axis=1,inplace=True)\ntest['adv_val'] =  0\n\ntrain = pd.read_csv('/kaggle/input/playground-series-s3e6/train.csv')\ntrain.drop(\"id\",axis=1,inplace=True)\ntrain['adv_val'] =  1\n\noriginal = pd.read_csv('/kaggle/input/paris-housing-price-prediction/ParisHousing.csv')\noriginal['adv_val'] =  2\n\nsample = pd.read_csv('/kaggle/input/playground-series-s3e6/sample_submission.csv')","metadata":{"papermill":{"duration":0.18808,"end_time":"2023-03-29T16:43:58.516597","exception":false,"start_time":"2023-03-29T16:43:58.328517","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-30T19:58:58.128699Z","iopub.execute_input":"2023-03-30T19:58:58.129077Z","iopub.status.idle":"2023-03-30T19:58:58.245538Z","shell.execute_reply.started":"2023-03-30T19:58:58.129046Z","shell.execute_reply":"2023-03-30T19:58:58.244556Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Engineering","metadata":{}},{"cell_type":"code","source":"#Define the target label\n\ntarget='price'\n\n#Create a dict - Why?\nsets={'train':train,'test':test,'original':original}","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:58.249972Z","iopub.execute_input":"2023-03-30T19:58:58.250265Z","iopub.status.idle":"2023-03-30T19:58:58.256030Z","shell.execute_reply.started":"2023-03-30T19:58:58.250238Z","shell.execute_reply":"2023-03-30T19:58:58.254885Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Function to drop rows that are duplicated in the train set and the original deck\n\ndef dropping_duplicates(data):\n    data.drop_duplicates(inplace = True)\n    \nfor n in {'train':train,'original':original}:\n    print(f\"\\033[0;33;40m A number of duplicated rows in {n} is {sets[n].duplicated().sum()}, they were dropped \\033[0;30;0m\")\n    dropping_duplicates(sets[n])","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:58.257878Z","iopub.execute_input":"2023-03-30T19:58:58.258583Z","iopub.status.idle":"2023-03-30T19:58:58.309077Z","shell.execute_reply.started":"2023-03-30T19:58:58.258545Z","shell.execute_reply":"2023-03-30T19:58:58.307873Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\u001b[0;33;40m A number of duplicated rows in train is 0, they were dropped \u001b[0;30;0m\n\u001b[0;33;40m A number of duplicated rows in original is 0, they were dropped \u001b[0;30;0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:58.310501Z","iopub.execute_input":"2023-03-30T19:58:58.311408Z","iopub.status.idle":"2023-03-30T19:58:59.131689Z","shell.execute_reply.started":"2023-03-30T19:58:58.311377Z","shell.execute_reply":"2023-03-30T19:58:59.130675Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"########################\n### General Settings ###\n########################\n\ngpu_switch = 'ON'\n\n###############################\n### RepeatedStratifiedKFold ###\n###############################\n\nn_splits = 5\nn_repeats =10\nsm=SMOTE(sampling_strategy='minority')\n#cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats = n_repeats, random_state=2023)\ncv = RepeatedKFold(n_splits=n_splits, n_repeats = n_repeats, random_state=2023)\n#cv = GroupKFold(n_splits=n_splits)\n\n########################\n### Define Weights   ###\n########################\n\n# Understand where and why the weights are used\nweights = {0: 0.5009553158705701, 1: 262.19354838709677}","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.135611Z","iopub.execute_input":"2023-03-30T19:58:59.135922Z","iopub.status.idle":"2023-03-30T19:58:59.141739Z","shell.execute_reply.started":"2023-03-30T19:58:59.135893Z","shell.execute_reply":"2023-03-30T19:58:59.140727Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### Define lists for feature engineering","metadata":{}},{"cell_type":"code","source":"num_cols = ['squareMeters','numberOfRooms','floors','cityPartRange','numPrevOwners',\n            'made','basement','attic','garage','hasGuestRoom','cityCode'\n            ]\ncat_cols = ['hasYard','hasPool','isNewBuilt','hasStormProtector','hasStorageRoom',]\ndrop_col = []","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.143302Z","iopub.execute_input":"2023-03-30T19:58:59.144392Z","iopub.status.idle":"2023-03-30T19:58:59.154930Z","shell.execute_reply.started":"2023-03-30T19:58:59.144351Z","shell.execute_reply":"2023-03-30T19:58:59.153842Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### Drop list from the test, train and original data set","metadata":{}},{"cell_type":"code","source":"#Drop the features in the list that have been define above\ntrain.drop(drop_col,axis=1,inplace=True)\ntest.drop(drop_col,axis=1,inplace=True)\noriginal.drop(drop_col,axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.158428Z","iopub.execute_input":"2023-03-30T19:58:59.158685Z","iopub.status.idle":"2023-03-30T19:58:59.171301Z","shell.execute_reply.started":"2023-03-30T19:58:59.158661Z","shell.execute_reply":"2023-03-30T19:58:59.170234Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Version 1\nnum_imp = SimpleImputer(strategy='mean')\ncat_imp = SimpleImputer(strategy='most_frequent')\nohe = OneHotEncoder(handle_unknown='ignore',sparse = False,drop=\"first\")\n\ntrain[num_cols] = pd.DataFrame(num_imp.fit_transform(train[num_cols]),columns=num_cols)\ntrain[cat_cols] = pd.DataFrame(cat_imp.fit_transform(train[cat_cols]),columns=cat_cols)\ntrain_temp=pd.DataFrame(ohe.fit_transform(train[cat_cols]),columns=ohe.get_feature_names_out())\ntrain=pd.concat([train.drop(cat_cols,axis=1),train_temp],axis=1) \n\noriginal[num_cols] = pd.DataFrame(num_imp.transform(original[num_cols]),columns=num_cols)\noriginal[cat_cols] = pd.DataFrame(cat_imp.transform(original[cat_cols]),columns=cat_cols)\noriginal_temp=pd.DataFrame(ohe.transform(original[cat_cols]),columns=ohe.get_feature_names_out())\noriginal=pd.concat([original.drop(cat_cols,axis=1),original_temp],axis=1)    \n\ntest[num_cols] = pd.DataFrame(num_imp.transform(test[num_cols]),columns=num_cols)\ntest[cat_cols] = pd.DataFrame(cat_imp.transform(test[cat_cols]),columns=cat_cols)\ntest_temp=pd.DataFrame(ohe.transform(test[cat_cols]),columns=ohe.get_feature_names_out())\ntest=pd.concat([test.drop(cat_cols,axis=1),test_temp],axis=1) ","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.176396Z","iopub.execute_input":"2023-03-30T19:58:59.176656Z","iopub.status.idle":"2023-03-30T19:58:59.294931Z","shell.execute_reply.started":"2023-03-30T19:58:59.176623Z","shell.execute_reply":"2023-03-30T19:58:59.293673Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Add improved version with Sklearn pipeline","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.296734Z","iopub.execute_input":"2023-03-30T19:58:59.298234Z","iopub.status.idle":"2023-03-30T19:58:59.305079Z","shell.execute_reply.started":"2023-03-30T19:58:59.298184Z","shell.execute_reply":"2023-03-30T19:58:59.303241Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Combine train and priginbal data after the adversarial cross-validation\ntrain=pd.concat([train,original],ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.307380Z","iopub.execute_input":"2023-03-30T19:58:59.309151Z","iopub.status.idle":"2023-03-30T19:58:59.319027Z","shell.execute_reply.started":"2023-03-30T19:58:59.309108Z","shell.execute_reply":"2023-03-30T19:58:59.317894Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Drop the adversarial column\ntrain=train.drop('adv_val',axis=1)\ntest=test.drop('adv_val',axis=1)\noriginal=original.drop('adv_val',axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.320611Z","iopub.execute_input":"2023-03-30T19:58:59.321059Z","iopub.status.idle":"2023-03-30T19:58:59.336814Z","shell.execute_reply.started":"2023-03-30T19:58:59.321023Z","shell.execute_reply":"2023-03-30T19:58:59.335911Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"#### Define the features and target labels","metadata":{}},{"cell_type":"code","source":"# Drop the target from the dataframe to create the features\nX = train.drop([target],axis=1)\n#Create a series with the targets\ny = train[target]\n#Shuffle the data\nX,y=shuffle(X,y,random_state=2023)\n#Reset the index for the feature dataframe and the target labels\nX = X.reset_index(drop=True)\ny = y.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.338171Z","iopub.execute_input":"2023-03-30T19:58:59.338591Z","iopub.status.idle":"2023-03-30T19:58:59.354617Z","shell.execute_reply.started":"2023-03-30T19:58:59.338554Z","shell.execute_reply":"2023-03-30T19:58:59.353524Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"###########################\n### additional dropping ###\n###########################\n\n#Drop outliers by row/sample/id\nindexies =X[X['squareMeters']>99999].index\nX=X.drop(indexies).reset_index(drop=True) #square meters\ny=y.drop(indexies).reset_index(drop=True) #square meters","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.355950Z","iopub.execute_input":"2023-03-30T19:58:59.356358Z","iopub.status.idle":"2023-03-30T19:58:59.370563Z","shell.execute_reply.started":"2023-03-30T19:58:59.356322Z","shell.execute_reply":"2023-03-30T19:58:59.369470Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"X['floors']=X['floors'].apply(lambda x: x if x<1000 else train['floors'].mean())\nX['made']=X['made'].apply(lambda x: x if x<2022 else train['made'].mean() )\nX['basement']=X['basement'].apply(lambda x: x if x<=10000 else train['basement'].mean() )\ntest['basement']=test['basement'].apply(lambda x: x if x<=10000 else train['basement'].mean() )\nX['attic']=X['attic'].apply(lambda x: x if x<=10000 else train['attic'].mean() )\ntest['attic']=test['attic'].apply(lambda x: x if x<=10000 else train['attic'].mean() )\nX['garage']=X['garage'].apply(lambda x: x if x<=1000 else train['garage'].mean() )\nX['cityCode']=X['cityCode'].apply(lambda x: x if x<=100000 else int(train['cityCode'].mean()) )\ntest['cityCode']=test['cityCode'].apply(lambda x: x if x<=100000 else int(train['cityCode'].mean()) )\nX['points']=X['hasYard_1']+X['hasPool_1']+X['isNewBuilt_1']+X['hasStormProtector_1']+X['hasStorageRoom_1']\nX.drop(['hasYard_1','hasPool_1','isNewBuilt_1','hasStormProtector_1','hasStorageRoom_1'],axis=1,inplace=True)\ntest['points']=test['hasYard_1']+test['hasPool_1']+test['isNewBuilt_1']+test['hasStormProtector_1']+test['hasStorageRoom_1']\ntest.drop(['hasYard_1','hasPool_1','isNewBuilt_1','hasStormProtector_1','hasStorageRoom_1'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.372082Z","iopub.execute_input":"2023-03-30T19:58:59.372587Z","iopub.status.idle":"2023-03-30T19:58:59.461866Z","shell.execute_reply.started":"2023-03-30T19:58:59.372550Z","shell.execute_reply":"2023-03-30T19:58:59.460874Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"X=X.reset_index(drop=True)\ny=y.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.463465Z","iopub.execute_input":"2023-03-30T19:58:59.464100Z","iopub.status.idle":"2023-03-30T19:58:59.470075Z","shell.execute_reply.started":"2023-03-30T19:58:59.464062Z","shell.execute_reply":"2023-03-30T19:58:59.468912Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def StaSca_transform(X,test):\n    StaSca = StandardScaler()\n    X[num_cols] = pd.DataFrame(data = StaSca.fit_transform(X[num_cols]),columns = X[num_cols].columns)\n    test[num_cols] = pd.DataFrame(data = StaSca.transform(test[num_cols]),columns = test[num_cols].columns)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.471559Z","iopub.execute_input":"2023-03-30T19:58:59.472473Z","iopub.status.idle":"2023-03-30T19:58:59.479200Z","shell.execute_reply.started":"2023-03-30T19:58:59.472436Z","shell.execute_reply":"2023-03-30T19:58:59.478258Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def Box_transform(X,test):\n    box_cols = ['fixed acidity','volatile acidity','residual sugar',\n                 'chlorides','free sulfur dioxide','total sulfur dioxide',\n                 'sulphates','alcohol']\n\n    for column in box_cols: \n        X_temp,fitted_lambda = stats.boxcox(X[column]) \n        X[column]=X_temp \n        test_temp = stats.boxcox(test[column],fitted_lambda) \n        test[column]=test_temp","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.482370Z","iopub.execute_input":"2023-03-30T19:58:59.483002Z","iopub.status.idle":"2023-03-30T19:58:59.489743Z","shell.execute_reply.started":"2023-03-30T19:58:59.482960Z","shell.execute_reply":"2023-03-30T19:58:59.488703Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def perm_imp(model,data,target):\n    X = data.to_numpy().copy()\n    y = target.to_numpy().copy()\n    permute = PermutationImportance(model,random_state=2023,n_iter =2,cv=10,scoring='neg_root_mean_squared_error').fit(X, y)\n    eli5.show_weights(permute, feature_names = data.columns.tolist(),top=50)\n    values = dict(zip(list(data.columns),list(permute.feature_importances_)))\n    sorted_dict = {}\n    sorted_keys = sorted(values, key=values.get)\n    for w in sorted_keys:\n        sorted_dict[w] = np.round(values[w],3)\n    return sorted_dict","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.491238Z","iopub.execute_input":"2023-03-30T19:58:59.491895Z","iopub.status.idle":"2023-03-30T19:58:59.501775Z","shell.execute_reply.started":"2023-03-30T19:58:59.491859Z","shell.execute_reply":"2023-03-30T19:58:59.500747Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"#Load the necessary packages\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nimport pandas as pd\nimport numpy as np\nimport base64\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport random\nimport gc\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.preprocessing import minmax_scaling\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom imblearn.over_sampling import SMOTE\nfrom scipy import stats\nimport optuna\nimport xgboost as xgb\nfrom sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,AdaBoostClassifier, GradientBoostingClassifier, \n                              ExtraTreesClassifier, VotingClassifier,ExtraTreesRegressor,AdaBoostRegressor,GradientBoostingRegressor)\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.utils import class_weight\nfrom catboost import CatBoostRegressor\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import SVR\nfrom sklearn import datasets, linear_model\nimport lightgbm as lgb\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.signal import argrelmin\nfrom scipy.stats import skew\nfrom scipy import stats\n\n\npd.set_option('display.max_columns', None)   \n\nfrom sklearn.model_selection import train_test_split\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport time\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense,BatchNormalization,Dropout\nfrom tensorflow.keras import utils\nimport tensorflow_addons as tfa\nimport keras_tuner\nfrom kerastuner.tuners import RandomSearch, Hyperband, BayesianOptimization\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom kerastuner.tuners import RandomSearch\nfrom kerastuner import HyperParameters, Objective","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:58:59.503261Z","iopub.execute_input":"2023-03-30T19:58:59.503783Z","iopub.status.idle":"2023-03-30T19:59:09.468277Z","shell.execute_reply.started":"2023-03-30T19:58:59.503690Z","shell.execute_reply":"2023-03-30T19:59:09.465908Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:78: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n","output_type":"stream"}]},{"cell_type":"code","source":"########################\n### Define Weights   ###\n########################\n\n\nclasses = y.unique()\nweight = class_weight.compute_class_weight(class_weight='balanced', classes=classes, y=y)\nweights = dict(zip(classes, list(weight)))\n\n#######################\n### Optuna Settings ###\n#######################\n\noptuna_study = True \n\noptuna_models={'LGBM' : True, \n               'XGB'  : False,\n               'CAT'  : False,\n               'AB'   : False,\n               'GB'   : False,\n               'ET'   : False,\n               'RF'   : False,\n               'LCV'  : False,\n               'LR'   : False,\n               'KNC'  : False,\n               'SVC'  : False,\n               'KERAS': False}\n\n#######################\n### Finish Settings ###\n#######################\nfinish_set = True\n\nfinish_models={'LGBM' : False,\n               'XGB'  : True,\n               'CAT'  : True,\n               'AB'   : False,\n               'GB'   : False,\n               'ET'   : False,\n               'RF'   : True,\n               'LCV'  : False,\n               'LR'   : False,\n               'KNC'  : False,\n               'SVC'  : False,\n               'KERAS': False}\n\n#####################\n### Consolidation ###\n#####################\nmodels={} # Dictionary to store the model\npreds_val={} # Dictionary to store the predictions on the valdation data set\npreds_test={} # Dictionary to store the predictions on the valdation test set\n\n#######################\n### X for modelling ###\n#######################\nX_model=X.copy() # This code also define a new variable before the modelling\ntest_model = test.copy()\ny_model = y.copy()","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:59:09.471042Z","iopub.execute_input":"2023-03-30T19:59:09.472118Z","iopub.status.idle":"2023-03-30T19:59:09.503322Z","shell.execute_reply.started":"2023-03-30T19:59:09.472076Z","shell.execute_reply":"2023-03-30T19:59:09.502367Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"################\n### Settings ###\n################\nmodel_name='LGBM'\nmetric = 'rmse'","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:59:09.504983Z","iopub.execute_input":"2023-03-30T19:59:09.505379Z","iopub.status.idle":"2023-03-30T19:59:09.511817Z","shell.execute_reply.started":"2023-03-30T19:59:09.505340Z","shell.execute_reply":"2023-03-30T19:59:09.510364Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"X = X_model\ntest = test_model\ny=y_model","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:59:09.513347Z","iopub.execute_input":"2023-03-30T19:59:09.513962Z","iopub.status.idle":"2023-03-30T19:59:09.520660Z","shell.execute_reply.started":"2023-03-30T19:59:09.513820Z","shell.execute_reply":"2023-03-30T19:59:09.519362Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#Define a list of features which should be dropped before fitting with LGBM\nLGBM_drop_list=[]","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:59:09.522143Z","iopub.execute_input":"2023-03-30T19:59:09.522566Z","iopub.status.idle":"2023-03-30T19:59:09.528464Z","shell.execute_reply.started":"2023-03-30T19:59:09.522529Z","shell.execute_reply":"2023-03-30T19:59:09.527438Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Apply the drop to the dataframe\nX = X_model.drop(LGBM_drop_list,axis=1)\ntest = test_model.drop(LGBM_drop_list,axis=1)\ny=y_model","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:59:09.529735Z","iopub.execute_input":"2023-03-30T19:59:09.530372Z","iopub.status.idle":"2023-03-30T19:59:09.541783Z","shell.execute_reply.started":"2023-03-30T19:59:09.530335Z","shell.execute_reply":"2023-03-30T19:59:09.540761Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"\n# code to decide if a GPU should be used or not\nif gpu_switch == \"ON\":\n    method = \"gpu\"\nelse:\n    method = \"cpu\"\n\ndef objective(trial):\n    param = {\n     'device': method,# The logic to use GPU or CPU is applied here\n     \"metric\":trial.suggest_categorical(\"metric\", [metric]), \n     'verbosity': -1,\n     'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0),\n     'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0),\n     'learning_rate': trial.suggest_float('learning_rate', 0.001,0.1),\n     'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n     'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n     'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n     'early_stopping_round' : trial.suggest_int('early_stopping_round', 300, 300),\n     'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n     'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n     'n_estimators' : trial.suggest_int('n_estimators', 100, 5000),\n     \"subsample\":trial.suggest_categorical(\"subsample\", [None]),\n     \"subsample_freq\":trial.suggest_categorical(\"subsample_freq\", [None]),\n     \"reg_alpha\":trial.suggest_categorical(\"reg_alpha\", [None]),\n     \"colsample_bytree\":trial.suggest_categorical(\"colsample_bytree\", [None]),\n     \"reg_lambda\":trial.suggest_categorical(\"reg_lambda\", [None]),\n     #'class_weight':weights\n             }\n    \n    results=[] # Create an empty list for the results\n    n_iterations=[] # Create an empty list for the iterations\n    for train_index, test_index in cv.split(X, y):\n        train_X, valid_X = X.iloc[train_index], X.iloc[test_index]\n        train_y, valid_y = y.iloc[train_index], y.loc[test_index]\n        model = lgb.LGBMRegressor(**param).fit(train_X,train_y,\n                                            eval_set=[(valid_X,valid_y)],\n                                            callbacks=[lgb.log_evaluation(period=0, show_stdv=False)]\n                                             )  \n        result = np.sqrt(mean_squared_error(valid_y,model.predict(valid_X)))\n        results.append(result)\n    n=sum(results)/len(results)   \n    return n\n\nif  optuna_study == optuna_models[model_name]:\n    study = optuna.create_study(pruner=optuna.pruners.HyperbandPruner(),\n                                direction='minimize')\n    study.optimize(objective, n_trials=10)\n    print('Best trial:', study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T19:59:09.543403Z","iopub.execute_input":"2023-03-30T19:59:09.543769Z","iopub.status.idle":"2023-03-30T20:04:23.492373Z","shell.execute_reply.started":"2023-03-30T19:59:09.543733Z","shell.execute_reply":"2023-03-30T20:04:23.490722Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2023-03-30 19:59:09,554]\u001b[0m A new study created in memory with name: no-name-c9f45f84-1c89-4d9b-a9e7-697fe69dab23\u001b[0m\n\u001b[33m[W 2023-03-30 20:04:23,068]\u001b[0m Trial 0 failed with parameters: {'metric': 'rmse', 'lambda_l1': 8.95642072588978, 'lambda_l2': 5.953923529793194, 'learning_rate': 0.023531618241183515, 'num_leaves': 219, 'feature_fraction': 0.732854489041163, 'bagging_fraction': 0.5889739061477223, 'early_stopping_round': 300, 'bagging_freq': 7, 'min_child_samples': 22, 'n_estimators': 1366, 'subsample': None, 'subsample_freq': None, 'reg_alpha': None, 'colsample_bytree': None, 'reg_lambda': None} because of the following error: KeyboardInterrupt().\u001b[0m\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n    value_or_values = func(trial)\n  File \"/tmp/ipykernel_93/2733413998.py\", line 37, in objective\n    callbacks=[lgb.log_evaluation(period=0, show_stdv=False)]\n  File \"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\", line 899, in fit\n    categorical_feature=categorical_feature, callbacks=callbacks, init_model=init_model)\n  File \"/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\", line 758, in fit\n    callbacks=callbacks\n  File \"/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py\", line 292, in train\n    booster.update(fobj=fobj)\n  File \"/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py\", line 3023, in update\n    ctypes.byref(is_finished)))\nKeyboardInterrupt\n\u001b[33m[W 2023-03-30 20:04:23,077]\u001b[0m Trial 0 failed with value None.\u001b[0m\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_93/2733413998.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     study = optuna.create_study(pruner=optuna.pruners.HyperbandPruner(),\n\u001b[1;32m     46\u001b[0m                                 direction='minimize')\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best trial:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         )\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_93/2733413998.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     35\u001b[0m         model = lgb.LGBMRegressor(**param).fit(train_X,train_y,\n\u001b[1;32m     36\u001b[0m                                             \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_stdv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                                              )  \n\u001b[1;32m     39\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0meval_init_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                     \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                     categorical_feature=categorical_feature, callbacks=callbacks, init_model=init_model)\n\u001b[0m\u001b[1;32m    900\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0minit_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m         )\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    290\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3021\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3022\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3023\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   3024\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"params_best = study.best_trial.params","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if finish_models[model_name] and finish_set:\n    results=[]\n    for i,(train_index, test_index) in enumerate(cv.split(X, y)):\n        j=i//n_splits\n        start_time = time.time()\n        train_X, valid_X = X.iloc[train_index], X.iloc[test_index]\n        train_y, valid_y = y.iloc[train_index], y.iloc[test_index]\n        model = lgb.LGBMRegressor(**params_best).fit(train_X,train_y,\n                                                eval_set=[(valid_X,valid_y)],\n                                                callbacks=[lgb.log_evaluation(period=0, show_stdv=False)])  \n        \n        models[f'{model_name}_{i}_{j}']=(model)\n        preds_val[f'{model_name}_{i}_{j}']=(model.predict(valid_X))\n        preds_test[f'{model_name}_{i}_{j}']=(model.predict(test))\n        \n        result = np.sqrt(mean_squared_error(valid_y,model.predict(valid_X)))\n        results.append(result)\n        print (f'\\033[0;33;40m Step #{i}.' + f\"--- {time.time() - start_time}s sec ---\" + f\"Auc result = {result} \\033[0;30;0m\")\n    print (f'\\033[0;35;40m Final LGB Result = {sum(results)/len(results)} \\033[0;30;0m')","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:04:23.493573Z","iopub.status.idle":"2023-03-30T20:04:23.494741Z","shell.execute_reply.started":"2023-03-30T20:04:23.494452Z","shell.execute_reply":"2023-03-30T20:04:23.494479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacking","metadata":{}},{"cell_type":"code","source":"for n in preds_val:\n    preds_val[n]=np.rint(preds_val[n])","metadata":{"execution":{"iopub.status.busy":"2023-03-30T20:06:16.954159Z","iopub.execute_input":"2023-03-30T20:06:16.954541Z","iopub.status.idle":"2023-03-30T20:06:16.959767Z","shell.execute_reply.started":"2023-03-30T20:06:16.954508Z","shell.execute_reply":"2023-03-30T20:06:16.958616Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"stack_preds=[]\nkey_model= list(preds_val.keys())[0].split(\"_\")[0]\nkey_last = list(preds_val.keys())[-1]\n\nfor step_model in np.arange(0,n_repeats,1):\n    key_model= list(preds_val.keys())[0].split(\"_\")[0]\n    temp_0 = pd.DataFrame()\n    temp_1 = pd.DataFrame()\n    for step_pred in preds_val:\n        key = int(step_pred.split(\"_\")[2])\n        if key == step_model and step_pred.split(\"_\")[0]==key_model:\n            temp_1=pd.concat([temp_1,pd.DataFrame(preds_val[step_pred])],axis=0)\n\n        else:\n            if key != step_model:\n                next\n\n            else:\n                temp_0=pd.concat([temp_0,temp_1],axis=1)\n                key_model=step_pred.split(\"_\")[0]\n                temp_1=pd.DataFrame()\n                temp_1=pd.concat([temp_1,pd.DataFrame(preds_val[step_pred])],axis=0)\n    if step_pred==key_last:\n        temp_0=pd.concat([temp_0,temp_1],axis=1)\n\n    stack_preds.append(temp_0.reset_index(drop=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for n in preds_test:\n    preds_test[n]=np.rint(preds_test[n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stack_tests=[]\nkey_model= list(preds_test.keys())[0].split(\"_\")[0]\nkey_last = list(preds_test.keys())[-1]\n\nfor step_model in np.arange(0,n_repeats,1):\n    key_model= list(preds_test.keys())[0].split(\"_\")[0]\n    print(step_model)\n    temp_0 = pd.DataFrame()\n    temp_1 = pd.DataFrame()\n    for step_pred in preds_test:\n        key = int(step_pred.split(\"_\")[2])\n        if key == step_model and step_pred.split(\"_\")[0]==key_model:\n            temp_1=pd.concat([temp_1,pd.DataFrame(preds_test[step_pred])],axis=1)\n\n        else:\n            if key != step_model:\n                next\n\n            else:\n                temp_1=temp_1.mean(axis=1)\n                temp_0=pd.concat([temp_0,temp_1],axis=1)\n            \n                key_model=step_pred.split(\"_\")[0]\n                temp_1=pd.DataFrame()\n                temp_1=pd.concat([temp_1,pd.DataFrame(preds_test[step_pred])],axis=1)\n    if step_pred==key_last:\n        temp_1=temp_1.mean(axis=1)\n        temp_0=pd.concat([temp_0,temp_1],axis=1)\n\n    stack_tests.append(temp_0.reset_index(drop=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stack_targets=[]\nfor step_model in np.arange(0,n_repeats,1):\n    temp=pd.DataFrame()\n    for i,(train_index, test_index) in enumerate(cv.split(X, y)):\n        key = i//n_splits\n        train_X, valid_X = X[train_index], X[test_index]\n        train_y, valid_y = y[train_index], y[test_index]\n        if step_model == key:\n            temp=pd.concat([temp,pd.DataFrame(valid_y)],axis=0)\n    stack_targets.append(temp.reset_index(drop=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_best= {'early_stopping_round':100, \n              'n_estimators':5000}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results=[]\npredicts=pd.DataFrame()\nfor step_model in np.arange(0,n_repeats,1):\n    print(step_model)\n    X_stack = stack_preds[step_model].to_numpy()\n    test_stack = stack_tests[step_model].to_numpy()\n    y_stack = stack_targets[step_model][0].to_numpy()\n    for i,(train_index, test_index) in enumerate(cv.split(X, y)):\n        temp=pd.DataFrame()\n        train_X, valid_X = X_stack[train_index], X_stack[test_index]\n        train_y, valid_y = y_stack[train_index], y_stack[test_index]\n        model = lgb.LGBMRegressor(**params_best).fit(train_X,train_y,\n                                                eval_set=[(valid_X,valid_y)],\n                                                callbacks=[lgb.log_evaluation(period=0, show_stdv=False)],\n                                                 )  \n        result = np.sqrt(mean_squared_error(valid_y,(model.predict(valid_X))))\n        results.append(result)\n        print (f'\\033[0;33;40m Step #{i}.' + f\"--- {time.time() - start_time}s seconds ---\" + f\"Auc result = {result} \\033[0;30;0m\")\n        predict = model.predict(test_stack)\n        temp[i]=predict\n    predicts[step_model]=temp.mean(axis=1)\n\nprint (f'\\033[0;35;40m Final LGB Result = {sum(results)/len(results)} \\033[0;30;0m')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample['price'] = predicts.mean(axis=1)\nsample.to_csv('Ensemble(Stacking).csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}